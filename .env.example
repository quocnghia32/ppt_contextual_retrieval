# API Keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
COHERE_API_KEY=your_cohere_api_key_here

# Pinecone Configuration
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=ppt-contextual-retrieval

# LangSmith (Optional - for debugging)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=ppt-contextual-retrieval

# Rate Limiting
MAX_REQUESTS_PER_MINUTE=50
MAX_TOKENS_PER_MINUTE=100000

# Model Configuration

# Embeddings (OpenAI)
EMBEDDING_MODEL=text-embedding-3-small

# Context Generation (Choose provider: "openai" or "anthropic")
CONTEXT_GENERATION_PROVIDER=openai
# For OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# For Anthropic: claude-3-haiku-20240307, claude-3-sonnet-20240229
CONTEXT_GENERATION_MODEL=gpt-4o-mini

# Answer Generation (Choose provider: "openai" or "anthropic")
ANSWER_GENERATION_PROVIDER=openai
# For OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# For Anthropic: claude-3-sonnet-20240229, claude-3-opus-20240229
ANSWER_GENERATION_MODEL=gpt-4o

# Vision Model (OpenAI only)
VISION_MODEL=gpt-4o-mini

# Caching Configuration
ENABLE_EMBEDDING_CACHE=true       # Cache embeddings to disk
ENABLE_LLM_CACHE=true              # Cache LLM responses
CACHE_IN_MEMORY=false              # Use in-memory cache (false = disk-based)

# Application Settings
MAX_CHUNK_SIZE=400
CHUNK_OVERLAP=50
TOP_K_RETRIEVAL=20
TOP_N_RERANK=5
