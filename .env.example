# API Keys - Required
PINECONE_API_KEY=your_pinecone_api_key_here

# Azure OpenAI (Recommended for Production)
AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small
AZURE_OPENAI_API_VERSION_EMBEDDING=
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o
AZURE_OPENAI_API_VERSION_CHAT=

# OpenAI (Alternative)
OPENAI_API_KEY=your_openai_api_key_here

# Optional Providers
XAI_API_KEY=your_xai_api_key_here  # For X.AI (Grok) models
COHERE_API_KEY=your_cohere_api_key_here  # For reranking

# Pinecone Configuration
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=ppt-contextual-retrieval

# LangSmith (Optional - for debugging)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=ppt-contextual-retrieval

# Rate Limiting
MAX_REQUESTS_PER_MINUTE=50
MAX_TOKENS_PER_MINUTE=100000

# Model Configuration

# Embeddings (OpenAI)
EMBEDDING_MODEL=text-embedding-3-small

# Context Generation (Choose provider: "azure" or "openai")
CONTEXT_GENERATION_PROVIDER=openai
# For Azure/OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
CONTEXT_GENERATION_MODEL=gpt-4o-mini

# Answer Generation (Choose provider: "azure", "openai", or "xai")
ANSWER_GENERATION_PROVIDER=azure
# For Azure/OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# For X.AI: grok-4-fast-reasoning
ANSWER_GENERATION_MODEL=gpt-4o

# Vision Model (Azure OpenAI or OpenAI)
# For Azure/OpenAI: gpt-4o, gpt-4o-mini
VISION_MODEL=gpt-4o-mini

# Caching Configuration
ENABLE_EMBEDDING_CACHE=true       # Cache embeddings to disk
ENABLE_LLM_CACHE=true              # Cache LLM responses
CACHE_IN_MEMORY=false              # Use in-memory cache (false = disk-based)

# Application Settings
MAX_CHUNK_SIZE=400
CHUNK_OVERLAP=50
TOP_K_RETRIEVAL=20
TOP_N_RERANK=5
